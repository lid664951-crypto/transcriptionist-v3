"""
Background worker utilities for Qt threading.
Provides base classes and helper functions for managing QThread workers.
"""

import asyncio
import logging
from pathlib import Path
from typing import Optional, Any, Callable

from PySide6.QtCore import QThread, QObject, Signal

logger = logging.getLogger(__name__)


class BaseWorker(QObject):
    """
    Base class for background workers.
    Provides common signals and cancellation support.
    
    Signals:
        finished: Emitted when work is completed successfully
        error: Emitted when an error occurs (with error message)
        progress: Emitted to report progress (current, total, message)
    """
    
    finished = Signal(object)  # Result data
    error = Signal(str)  # Error message
    progress = Signal(int, int, str)  # current, total, message
    
    def __init__(self, parent: Optional[QObject] = None):
        super().__init__(parent)
        self._cancelled = False
    
    def cancel(self) -> None:
        """Request cancellation of the work."""
        self._cancelled = True
    
    @property
    def is_cancelled(self) -> bool:
        """Check if cancellation was requested."""
        return self._cancelled
    
    def run(self) -> None:
        """
        Override this method to implement the actual work.
        Check self.is_cancelled periodically and return early if True.
        """
        raise NotImplementedError("Subclasses must implement run()")


def cleanup_thread(
    thread: Optional[QThread],
    worker: Optional[QObject] = None,
    timeout_ms: int = 5000
) -> None:
    """
    Safely cleanup a QThread and its worker.
    
    Args:
        thread: The QThread to cleanup
        worker: Optional worker object (will be set to None after cleanup)
        timeout_ms: Timeout in milliseconds to wait for thread to finish
    """
    if thread is None:
        return
    
    try:
        if thread.isRunning():
            thread.quit()
            if not thread.wait(timeout_ms):
                logger.warning(f"Thread did not finish within {timeout_ms}ms, forcing termination")
                thread.terminate()
                thread.wait()
    except RuntimeError:
        # Thread already deleted
        pass


class DatabaseLoadWorker(BaseWorker):
    """
    Worker for loading audio files from database asynchronously.
    Used by LibraryPage to avoid blocking UI on startup.
    """
    
    def __init__(self, parent: Optional[QObject] = None):
        super().__init__(parent)
    
    def run(self) -> None:
        """Load audio files from database."""
        try:
            from transcriptionist_v3.infrastructure.database.connection import get_session
            from transcriptionist_v3.infrastructure.database.models import AudioFile
            from transcriptionist_v3.domain.models.metadata import AudioMetadata
            from pathlib import Path
            
            session = get_session()
            try:
                from sqlalchemy.orm import joinedload
                from transcriptionist_v3.infrastructure.database.models import LibraryPath
                
                # Query all audio files with tags eagerly loaded
                audio_files = session.query(AudioFile).options(joinedload(AudioFile.tags)).all()
                
                if not audio_files:
                    logger.info("No audio files in database")
                    # Fetch library paths (roots)
                    lib_paths = session.query(LibraryPath).filter_by(enabled=True).all()
                    root_paths = [Path(lp.path) for lp in lib_paths]
                    
                    self.finished.emit(([], root_paths))
                    return
                
                total = len(audio_files)
                results = []
                
                # OPTIMIZATION: Skip file existence check on startup for performance
                # Files will be validated when actually accessed (play, rename, etc.)
                # This makes startup 10-20x faster for large libraries
                
                for i, db_file in enumerate(audio_files):
                    if self.is_cancelled:
                        return
                    
                    file_path = Path(db_file.file_path)
                    
                    # Create metadata object (no file I/O)
                    metadata = AudioMetadata(
                        id=db_file.id,
                        duration=db_file.duration,
                        sample_rate=db_file.sample_rate,
                        bit_depth=db_file.bit_depth,
                        channels=db_file.channels,
                        format=db_file.format,
                        comment=getattr(db_file, 'description', '')  # Use comment field
                    )
                    
                    # Populate additional metadata
                    metadata.original_filename = getattr(db_file, 'original_filename', file_path.name)
                    metadata.tags = [t.tag for t in db_file.tags]
                    
                    results.append((file_path, metadata))
                    
                    # Report progress every 100 files (less frequent for speed)
                    if (i + 1) % 100 == 0 or i == total - 1:
                        self.progress.emit(i + 1, total, f"åŠ è½½ä¸­ ({i+1}/{total})")
                        
                # Fetch library paths (roots) at the end
                lib_paths = session.query(LibraryPath).filter_by(enabled=True).all()
                root_paths = [Path(lp.path) for lp in lib_paths]
                
                self.finished.emit((results, root_paths))
                
            finally:
                session.close()
                
        except Exception as e:
            logger.error(f"Failed to load from database: {e}")
            self.error.emit(str(e))


class TranslateWorker(BaseWorker):
    """
    Worker for AI translation tasks.
    Runs translation in background thread with progress reporting.
    """
    
    def __init__(
        self,
        files: list,
        api_key: str,
        model_config: dict,
        glossary: dict,
        template_id: str = "translated_only",
        source_lang: str = "è‡ªåŠ¨æ£€æµ‹",
        target_lang: str = "ç®€ä½“ä¸­æ–‡",
        parent: Optional[QObject] = None
    ):
        super().__init__(parent)
        self.files = files
        self.api_key = api_key
        self.model_config = model_config
        self.glossary = glossary
        self.template_id = template_id
        self.source_lang = source_lang
        self.target_lang = target_lang
        
        logger.info(f"TranslateWorker initialized with {len(self.files)} files, api_key={'yes' if self.api_key else 'no'}, {source_lang} -> {target_lang}")
    
    def run(self) -> None:
        """Execute the translation task."""
        from transcriptionist_v3.application.naming_manager.cleaning import CleaningManager
        
        # 1. é¢„å¤„ç†ï¼šåº”ç”¨æ¸…æ´—è§„åˆ™
        cleaning_manager = CleaningManager.instance()
        cleaned_files = []
        for fp in self.files:
            p = Path(fp)
            # æ¸…ç†æ–‡ä»¶åï¼ˆä¸å«åŽç¼€éƒ¨åˆ†ï¼‰
            cleaned_stem = cleaning_manager.apply_all(p.stem)
            cleaned_files.append(cleaned_stem + p.suffix)
        
        total = len(self.files)
        results = []
        
        # If API key provided, try AI translation
        if self.api_key:
            logger.info("Attempting AI translation...")
            try:
                from transcriptionist_v3.application.ai_engine.providers.openai_compatible import OpenAICompatibleService
                from transcriptionist_v3.application.ai_engine.base import AIServiceConfig
                
                # 2. å‡†å¤‡åŠ¨æ€ System Prompt
                # åªæœ‰ UCS æ ‡å‡†å‘½åéœ€è¦ Expert æ¨¡å¼
                needs_ucs = (self.template_id == "ucs_standard")
                logger.info(f"Building prompt for template='{self.template_id}', needs_ucs={needs_ucs}")
                
                # æž„å»ºåŠ¨æ€è¯­è¨€æç¤ºè¯ (ä¸å†åŒ…å«æœ¯è¯­åº“)
                custom_prompt = self._build_dynamic_prompt(needs_ucs)
                
                # Create service config
                service_config = AIServiceConfig(
                    provider_id=self.model_config["provider"],
                    api_key=self.api_key,
                    base_url=self.model_config["base_url"],
                    model_name=self.model_config["model"],
                    system_prompt=custom_prompt,
                    timeout=180,
                    max_tokens=4096,
                    temperature=0.3,
                )
                service = OpenAICompatibleService(service_config)
                
                # ä½¿ç”¨æ¸…æ´—åŽçš„æ–‡ä»¶åè¿›è¡Œç¿»è¯‘
                logger.info(f"Translating {len(cleaned_files)} cleaned filenames")
                
                # Run async translation
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                def progress_cb(current: int, total: int, msg: str) -> None:
                    if not self.is_cancelled:
                        self.progress.emit(current, total, msg)
                
                try:
                    result = loop.run_until_complete(
                        service.translate_batch(cleaned_files, progress_callback=progress_cb)
                    )
                    
                    if result.success and result.data:
                        logger.info(f"AI returned {len(result.data)} results")
                        for i, tr in enumerate(result.data):
                            # CRITICAL FIX: Strip extension immediately from AI result logic
                            # This ensures that 'translated' is always just the name, confirming to template expectations
                            original_name = Path(self.files[i]).name
                            original_suffix = Path(self.files[i]).suffix
                            
                            clean_translated = tr.translated.strip() if tr.translated else ""
                            # Remove extension if AI added it (case-insensitive)
                            if clean_translated.lower().endswith(original_suffix.lower()):
                                clean_translated = clean_translated[:-len(original_suffix)].strip()
                                logger.info(f"Stripped extension: '{tr.translated}' -> '{clean_translated}'")
                            
                            results.append({
                                'original': original_name,
                                'translated': clean_translated,
                                'category': tr.category,
                                'subcategory': tr.subcategory,
                                'descriptor': tr.descriptor,
                                'variation': tr.variation,
                                'file_path': self.files[i],
                                'status': 'å¾…åº”ç”¨'
                            })
                        self.finished.emit(results)
                        return
                    else:
                        logger.warning(f"AI translation failed: {result.error}, falling back to local")
                        
                finally:
                    loop.run_until_complete(service.cleanup())
                    loop.close()
                    
            except Exception as e:
                logger.error(f"AI translation error: {e}")
                import traceback
                logger.error(traceback.format_exc())
                # Fall through to local translation
        
        # Fallback to local glossary translation
        logger.info("Using local glossary translation")
        results = self._local_translate(total)
        logger.info(f"Local translation done, emitting finished with {len(results)} results")
        self.finished.emit(results)
    
    def _build_dynamic_prompt(self, needs_ucs: bool) -> str:
        """
        æž„å»ºåŠ¨æ€æç¤ºè¯ï¼Œä½¿ç”¨ç»Ÿä¸€æ¨¡æ¿å¡«å……æ‰€æœ‰å ä½ç¬¦
        
        Args:
            needs_ucs: æ˜¯å¦éœ€è¦ UCS ä¸“å®¶æ¨¡å¼
        """
        from transcriptionist_v3.application.ai_engine.providers.openai_compatible import (
            BASIC_TRANSLATION_PROMPT,
            EXPERT_UCS_PROMPT,
            LANGUAGE_ENFORCEMENT_TEMPLATES,
            TARGET_LANG_EXAMPLES,
        )
        
        # è¯­è¨€æ˜ å°„ï¼ˆç•Œé¢æ˜¾ç¤º -> AIç†è§£ï¼‰
        lang_map = {
            "è‡ªåŠ¨æ£€æµ‹": "auto-detect",
            "è‹±è¯­": "English",
            "æ—¥è¯­": "Japanese",
            "éŸ©è¯­": "Korean",
            "ä¿„è¯­": "Russian",
            "å¾·è¯­": "German",
            "æ³•è¯­": "French",
            "è¥¿ç­ç‰™è¯­": "Spanish",
            "ç®€ä½“ä¸­æ–‡": "Simplified Chinese",
            "ç¹ä½“ä¸­æ–‡": "Traditional Chinese"
        }
        
        source = lang_map.get(self.source_lang, "auto-detect")
        target = lang_map.get(self.target_lang, "Simplified Chinese")
        
        # èŽ·å–ç›®æ ‡è¯­è¨€ç¤ºä¾‹
        target_example = TARGET_LANG_EXAMPLES.get(target, "Translation")
        
        # 1. å¡«å……æºè¯­è¨€ï¼ˆè‡ªåŠ¨æ£€æµ‹æ—¶ä¸å†™æ­»ï¼‰
        if source == "auto-detect":
            source_lang_text = ""  # ä¸æŒ‡å®šï¼Œè®© AI è‡ªåŠ¨æ£€æµ‹
        else:
            source_lang_text = f"{source} "  # ä¾‹å¦‚ "English "
        
        # 2. å¡«å……è¯­è¨€å¼ºåˆ¶æŒ‡ä»¤
        language_enforcement = LANGUAGE_ENFORCEMENT_TEMPLATES.get(target, "")
        
        # 3. é€‰æ‹©å¹¶æž„å»ºæœ€ç»ˆæç¤ºè¯ (Dual Mode Logic)
        if needs_ucs:
            # Expert UCS Mode
            prompt = EXPERT_UCS_PROMPT
        else:
            # Basic Mode
            prompt = BASIC_TRANSLATION_PROMPT
            
        # 4. æ›¿æ¢é€šç”¨å ä½ç¬¦
        prompt = prompt.replace("{{SOURCE_LANG}}", source_lang_text)
        prompt = prompt.replace("{{TARGET_LANG}}", target)
        prompt = prompt.replace("{{TARGET_LANG_EXAMPLE}}", target_example)
        prompt = prompt.replace("{{LANGUAGE_ENFORCEMENT}}", language_enforcement)
        
        return prompt
    
    def _local_translate(self, total: int) -> list:
        """Fallback to local glossary translation."""
        import re
        from transcriptionist_v3.application.naming_manager.cleaning import CleaningManager
        
        results = []
        
        for i, file_path_str in enumerate(self.files):
            if self.is_cancelled:
                return results
            
            file_path = Path(file_path_str)
            original_name = file_path.name
            
            # ä½¿ç”¨æ¸…æ´—åŽçš„ä¸»æ–‡ä»¶å
            cleaning_manager = CleaningManager.instance()
            cleaned_stem = cleaning_manager.apply_all(file_path.stem)
            
            # Translate using glossary
            translated = self._translate_with_glossary(cleaned_stem) + file_path.suffix
            
            results.append({
                'original': original_name,
                'translated': translated,
                'file_path': file_path_str,
                'status': 'å¾…åº”ç”¨'
            })
            
            # Update progress
            self.progress.emit(i + 1, total, f"ç¿»è¯‘ä¸­: {original_name}")
            
            # Add a small delay for visual feedback if local translation is too fast
            import time
            time.sleep(0.01)
        
        return results
    
    def _translate_with_glossary(self, text: str) -> str:
        """ä½¿ç”¨æœ¯è¯­åº“ç¿»è¯‘æ–‡æœ¬ï¼Œæ”¯æŒCamelCaseæ‹†åˆ†å’Œå•å¤æ•°åŒ¹é…ã€‚"""
        import re
        
        # 1. å¯¹åŽŸå§‹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼šæ‹†åˆ† CamelCase å’Œä¸‹åˆ’çº¿
        # ä¾‹å¦‚: ClockTicking -> Clock Ticking, foot_step -> foot step
        parts = self._split_text(text)
        translated_parts = []
        
        for part in parts:
            if not part.strip():
                continue
            
            # å°è¯•ç¿»è¯‘è¯¥éƒ¨åˆ†
            translated_part = self._match_term(part)
            translated_parts.append(translated_part)
        
        # é‡æ–°ç»„åˆï¼ˆä¸­æ–‡ä¹‹é—´ä¸éœ€è¦ç©ºæ ¼ï¼Œè‹±æ–‡å’Œæ•°å­—ä¿ç•™åŽŸæ ·ï¼‰
        result = "".join(translated_parts)
        
        # å¦‚æžœç¿»è¯‘ç»“æžœå’ŒåŽŸåä¸€æ ·ä¸”åŒ…å«è¿žå­—ç¬¦/ä¸‹åˆ’çº¿ï¼Œå°è¯•ç›´æŽ¥å¯¹å…¨åè¿›è¡Œæœ¯è¯­æ›¿æ¢
        if result == text:
            sorted_terms = sorted(self.glossary.items(), key=lambda x: len(x[0]), reverse=True)
            for en_term, zh_term in sorted_terms:
                pattern = re.compile(re.escape(en_term), re.IGNORECASE)
                result = pattern.sub(zh_term, result)
        
        # æ¸…ç†å¤šä½™ç©ºæ ¼
        result = result.replace('_', ' ').strip()
        return result

    def _split_text(self, text: str) -> list:
        """å°†æ–‡æœ¬æ‹†åˆ†ä¸ºå•è¯ã€æ•°å­—å’Œç¬¦å·ã€‚æ”¯æŒCamelCaseã€‚"""
        import re
        # åŒ¹é…å¤§å†™å­—æ¯å‰çš„ç©ºéš™è¿›è¡Œæ‹†åˆ† (CamelCase)
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1 \\2', text)
        s2 = re.sub('([a-z0-9])([A-Z])', r'\\1 \\2', s1)
        # æ›¿æ¢ä¸‹åˆ’çº¿å’Œè¿žå­—ç¬¦ä¸ºç©ºæ ¼
        s3 = s2.replace('_', ' ').replace('-', ' ')
        # æŒ‰ç©ºæ ¼æ‹†åˆ†
        return s3.split()

    def _match_term(self, word: str) -> str:
        """åœ¨æœ¯è¯­åº“ä¸­åŒ¹é…å•ä¸ªå•è¯ï¼ŒåŒ…å«ç®€å•çš„å•å¤æ•°å¤„ç†ã€‚"""
        import re
        
        word_lower = word.lower()
        
        # 1. ç²¾ç¡®åŒ¹é…ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
        for en_term, zh_term in self.glossary.items():
            if en_term.lower() == word_lower:
                return zh_term
        
        # 2. ç®€å•çš„å¤æ•°åŒ¹é… (å¦‚æžœ word æ˜¯å•æ•°ï¼Œå°è¯•åœ¨æœ¯è¯­åº“æ‰¾å¤æ•°)
        # ä¾‹å¦‚: word='Clock', glossary has 'CLOCKS'
        plural_word = word_lower + 's'
        for en_term, zh_term in self.glossary.items():
            if en_term.lower() == plural_word:
                return zh_term
        
        # 3. å¦‚æžœ word ä»¥ 's' ç»“å°¾ï¼Œå°è¯•æ‰¾å•æ•°
        if word_lower.endswith('s') and len(word_lower) > 3:
            singular_word = word_lower[:-1]
            for en_term, zh_term in self.glossary.items():
                if en_term.lower() == singular_word:
                    return zh_term
        
        return word  # æ²¡åŒ¹é…åˆ°è¿”å›žåŽŸè¯


class ModelDownloadWorker(BaseWorker):
    """
    Worker for downloading AI models (e.g., CLAP) from Hugging Face Mirror.
    """
    
    BASE_URL = "https://hf-mirror.com/Xenova/clap-htsat-unfused/resolve/main"
    
    # Files required for CLAP ONNX inference
    FILES_TO_DOWNLOAD = [
        "onnx/model.onnx",
        "tokenizer.json",
        "vocab.json",
        "config.json",
        "preprocessor_config.json",
        "special_tokens_map.json"
    ]
    
    def __init__(self, save_dir: str, parent: Optional[QObject] = None):
        super().__init__(parent)
        self.save_dir = Path(save_dir)
    
    def run(self) -> None:
        try:
            import requests
            
            if not self.save_dir.exists():
                self.save_dir.mkdir(parents=True, exist_ok=True)
                
            total_files = len(self.FILES_TO_DOWNLOAD)
            
            # Calculate total size if possible or just count files
            # For better UX, we download sequentially
            
            for i, filename in enumerate(self.FILES_TO_DOWNLOAD):
                if self.is_cancelled:
                    return

                url = f"{self.BASE_URL}/{filename}"
                target_path = self.save_dir / filename
                
                # Ensure subdirectory exists (e.g. onnx/)
                target_path.parent.mkdir(parents=True, exist_ok=True)
                
                self.progress.emit(0, 100, f"æ­£åœ¨ä¸‹è½½: {filename}...")
                
                try:
                    response = requests.get(url, stream=True, timeout=30)
                    response.raise_for_status()
                    
                    total_size = int(response.headers.get('content-length', 0))
                    downloaded_size = 0
                    
                    with open(target_path, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            if self.is_cancelled:
                                return
                            if chunk:
                                f.write(chunk)
                                downloaded_size += len(chunk)
                                if total_size > 0:
                                    percent = int((downloaded_size / total_size) * 100)
                                    # Update detailed progress for large files
                                    if "model.onnx" in filename: 
                                         self.progress.emit(percent, 100, f"æ­£åœ¨ä¸‹è½½æ¨¡åž‹ä¸»ä½“ ({percent}%)...")
                    
                    logger.info(f"Downloaded {filename}")
                    
                except Exception as e:
                    logger.error(f"Failed to download {filename}: {e}")
                    self.error.emit(f"ä¸‹è½½å¤±è´¥: {filename} - {str(e)}")
                    return
            
            self.finished.emit(str(self.save_dir))
            
        except Exception as e:
            logger.error(f"Download process error: {e}")
            self.error.emit(str(e))


class CLAPIndexingWorker(BaseWorker):
    """
    Worker for computing CLAP embeddings for a list of files.
    OPTIMIZED: Uses batch processing for better GPU utilization.
    """
    
    def __init__(self, engine, file_paths: list, parent: Optional[QObject] = None):
        super().__init__(parent)
        self.engine = engine
        self.file_paths = file_paths
        
    def run(self) -> None:
        try:
            results = {} # {path: embedding}
            total = len(self.file_paths)
            
            # Disable Numba debug logging to avoid console spam
            import os
            os.environ['NUMBA_DISABLE_JIT'] = '0'  # Keep JIT enabled
            os.environ['NUMBA_DEBUG'] = '0'  # Disable debug output
            os.environ['NUMBA_DEBUGINFO'] = '0'
            
            # Show initialization progress
            self.progress.emit(0, total, "æ­£åœ¨åˆå§‹åŒ– AI æ¨¡åž‹...")
            
            # Ensure engine is ready
            if not self.engine.initialize():
                self.error.emit("CLAP æ¨¡åž‹åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·åœ¨è®¾ç½®ä¸­æ£€æŸ¥æ¨¡åž‹æ˜¯å¦ä¸‹è½½")
                return
            
            # ä»Žé…ç½®è¯»å– batch_size
            from transcriptionist_v3.core.config import AppConfig
            batch_size = AppConfig.get("ai.batch_size", 4)
            logger.info(f"Using batch_size from config: {batch_size}")
            
            # First batch takes longer due to Numba JIT compilation
            self.progress.emit(0, total, "æ­£åœ¨é¢„çƒ­éŸ³é¢‘å¤„ç†å¼•æ“Žï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦10-30ç§’ï¼‰...")
            
            # Process in batches
            processed = 0
            for i in range(0, total, batch_size):
                if self.is_cancelled:
                    return
                
                batch_paths = self.file_paths[i:i + batch_size]
                batch_num = i // batch_size + 1
                total_batches = (total + batch_size - 1) // batch_size
                
                # Update progress
                if i == 0:
                    self.progress.emit(i, total, f"æ­£åœ¨ç¼–è¯‘éŸ³é¢‘å¤„ç†å‡½æ•°ï¼ˆé¦–æ¬¡æ‰¹æ¬¡ï¼‰...")
                else:
                    self.progress.emit(processed, total, f"æ­£åœ¨æ‰¹é‡å¤„ç† ({batch_num}/{total_batches}): {len(batch_paths)} ä¸ªæ–‡ä»¶...")
                
                try:
                    # Batch processing - much faster!
                    batch_results = self.engine.get_audio_embeddings_batch(batch_paths, batch_size=batch_size)
                    results.update(batch_results)
                    processed += len(batch_results)
                    
                except Exception as e:
                    logger.warning(f"Batch processing failed for batch {batch_num}: {e}")
                    # Fall back to individual processing for this batch
                    for path in batch_paths:
                        try:
                            embedding = self.engine.get_audio_embedding(str(path))
                            if embedding is not None:
                                results[str(path)] = embedding
                                processed += 1
                        except Exception as e2:
                            logger.warning(f"Failed to embed {path}: {e2}")
                    
            self.finished.emit(results)
            
        except Exception as e:
            logger.error(f"Indexing error: {e}")
            self.error.emit(str(e))


class TaggingWorker(BaseWorker):
    """
    Worker for AI tagging tasks.
    Runs tagging in background thread with progress reporting.
    """
    
    log_message = Signal(str)  # æ—¥å¿—æ¶ˆæ¯ä¿¡å·
    batch_completed = Signal(list)  # æ‰¹æ¬¡å®Œæˆä¿¡å·
    
    def __init__(
        self,
        engine,
        selected_files: list,
        audio_embeddings: dict,
        tag_embeddings: dict,
        tag_list: list,
        tag_matrix,
        tag_translations: dict,
        parent: Optional[QObject] = None
    ):
        super().__init__(parent)
        self.engine = engine
        self.selected_files = selected_files
        self.audio_embeddings = audio_embeddings
        self.tag_embeddings = tag_embeddings
        self.tag_list = tag_list
        self.tag_matrix = tag_matrix
        self.tag_translations = tag_translations
        
    def run(self) -> None:
        """Execute the tagging task."""
        import numpy as np
        from pathlib import Path
        from transcriptionist_v3.infrastructure.database.connection import session_scope
        from transcriptionist_v3.infrastructure.database.models import AudioFile, AudioFileTag
        
        BATCH_SIZE = 10  # æ¯æ‰¹å¤„ç† 10 ä¸ªæ–‡ä»¶
        LOG_INTERVAL = 50  # æ¯ 50 ä¸ªæ–‡ä»¶æ›´æ–°ä¸€æ¬¡æ—¥å¿—
        UI_UPDATE_INTERVAL = 50  # æ¯ 50 ä¸ªæ–‡ä»¶åˆ·æ–°ä¸€æ¬¡ UI
        
        processed = 0
        batch_updates = []
        total = len(self.selected_files)
        
        try:
            with session_scope() as session:
                for i, file_path_str in enumerate(self.selected_files):
                    if self.is_cancelled:
                        return
                    
                    path_obj = Path(file_path_str)
                    
                    # 1. Get Audio Embedding
                    key_str = str(path_obj)
                    embedding = self.audio_embeddings.get(key_str)
                    
                    if embedding is None:
                        # å¦‚æžœæ²¡æœ‰ embeddingï¼Œè·³è¿‡
                        self.log_message.emit(f"âŒ è·³è¿‡ï¼ˆæ— ç´¢å¼•ï¼‰: {path_obj.name}")
                        continue
                    
                    # 2. Vectorized Classification
                    norm_audio = np.linalg.norm(embedding)
                    if norm_audio > 0:
                        embedding_norm = embedding / norm_audio
                    else:
                        embedding_norm = embedding
                    
                    # Dot product (Cosine Similarity)
                    scores = np.dot(self.tag_matrix, embedding_norm)
                    
                    # Top K
                    top_k_indices = np.argsort(scores)[::-1][:3]
                    top_tags = [self.tag_list[idx] for idx in top_k_indices]
                    
                    # 3. Process Tags (LLM Translation)
                    final_tags = []
                    for tag_en in top_tags:
                        # Check cache
                        if tag_en in self.tag_translations:
                            final_tags.append(self.tag_translations[tag_en])
                            continue
                        
                        # Call LLM (åŒæ­¥ï¼Œä½†åœ¨åŽå°çº¿ç¨‹ä¸é˜»å¡ž UI)
                        translated = self._translate_text_sync(tag_en, target_lang="zh")
                        if translated:
                            self.tag_translations[tag_en] = translated
                            final_tags.append(translated)
                        else:
                            final_tags.append(tag_en)  # Fallback
                    
                    # 4. Save to DB
                    db_file = session.query(AudioFile).filter_by(file_path=str(path_obj)).first()
                    if db_file:
                        session.query(AudioFileTag).filter_by(audio_file_id=db_file.id).delete()
                        for tag in final_tags:
                            new_tag = AudioFileTag(audio_file_id=db_file.id, tag=tag)
                            session.add(new_tag)
                        
                        # è®°å½•å¾…æ›´æ–°çš„æ–‡ä»¶
                        batch_updates.append({
                            'file_path': str(path_obj),
                            'tags': final_tags
                        })
                    
                    processed += 1
                    
                    # 5. æ¯ LOG_INTERVAL ä¸ªæ–‡ä»¶æ›´æ–°ä¸€æ¬¡æ—¥å¿—
                    if (i + 1) % LOG_INTERVAL == 0 or i == 0:
                        self.log_message.emit(f"å·²å¤„ç† {i+1}/{total} ä¸ªæ–‡ä»¶")
                    
                    # 6. æ¯ BATCH_SIZE ä¸ªæ–‡ä»¶æäº¤ä¸€æ¬¡æ•°æ®åº“
                    if (i + 1) % BATCH_SIZE == 0 or (i + 1) == total:
                        # æ‰¹é‡æäº¤åˆ°æ•°æ®åº“
                        session.commit()
                        
                        # 7. æ¯ UI_UPDATE_INTERVAL ä¸ªæ–‡ä»¶å‘é€ä¸€æ¬¡æ‰¹æ¬¡ä¿¡å·
                        if (i + 1) % UI_UPDATE_INTERVAL == 0 or (i + 1) == total:
                            # å‘é€æ‰¹é‡åˆ·æ–°ä¿¡å·
                            self.batch_completed.emit(batch_updates.copy())
                            self.log_message.emit(f"ðŸ’¾ å·²ä¿å­˜ {len(batch_updates)} ä¸ªæ–‡ä»¶çš„æ ‡ç­¾")
                            # æ¸…ç©ºæ‰¹æ¬¡ç¼“å­˜
                            batch_updates.clear()
                    
                    # 8. æ›´æ–°è¿›åº¦
                    self.progress.emit(i + 1, total, f"æ­£åœ¨å¤„ç†: {path_obj.name}")
                
                self.log_message.emit(f"\nðŸŽ‰ ä»»åŠ¡å®Œæˆï¼æˆåŠŸå¤„ç† {processed} ä¸ªæ–‡ä»¶ã€‚")
                self.finished.emit({'processed': processed, 'total': total})
                
        except Exception as e:
            logger.error(f"Tagging error: {e}", exc_info=True)
            self.error.emit(str(e))
    
    def _translate_text_sync(self, text: str, target_lang: str = "en") -> str:
        """Synchronously translate text"""
        from transcriptionist_v3.core.config import AppConfig
        from transcriptionist_v3.application.ai_engine.providers.openai_compatible import OpenAICompatibleService
        from transcriptionist_v3.application.ai_engine.base import AIServiceConfig
        import asyncio
        
        api_key = AppConfig.get("ai.api_key", "").strip()
        if not api_key:
            return None
        
        model_idx = AppConfig.get("ai.model_index", 0)
        model_configs = {
            0: {"provider": "deepseek", "model": "deepseek-chat", "base_url": "https://api.deepseek.com/v1"},
            1: {"provider": "openai", "model": "gpt-4o-mini", "base_url": "https://api.openai.com/v1"},
            2: {"provider": "doubao", "model": "doubao-pro-4k", "base_url": "https://ark.cn-beijing.volces.com/api/v3"},
        }
        config = model_configs.get(model_idx, model_configs[0])
        
        if target_lang == "zh":
            sys_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å½±è§†éŸ³æ•ˆæ ‡ç­¾ç¿»è¯‘ä¸“å®¶ã€‚

### ä»»åŠ¡
å°†ä»¥ä¸‹è‹±æ–‡éŸ³æ•ˆæ ‡ç­¾ç¿»è¯‘ä¸ºç®€æ´ã€é€šä¿—æ˜“æ‡‚çš„ä¸­æ–‡ã€‚

### ç¿»è¯‘åŽŸåˆ™
1. **å£è¯­åŒ–ä¼˜å…ˆ**ï¼šä½¿ç”¨å½±è§†åŽæœŸåˆ¶ä½œäººå‘˜æ—¥å¸¸ä½¿ç”¨çš„è¯´æ³•ï¼Œé¿å…ç”Ÿç¡¬çš„ç›´è¯‘
2. **ç®€æ´æ˜Žäº†**ï¼šä¼˜å…ˆä½¿ç”¨2-4ä¸ªå­—çš„ç®€çŸ­è¯æ±‡ï¼Œè®©ç”¨æˆ·ä¸€çœ¼å°±èƒ½çœ‹æ‡‚
3. **è¡Œä¸šä¹ æƒ¯**ï¼šéµå¾ªä¸­æ–‡å½±è§†éŸ³æ•ˆè¡Œä¸šçš„å¸¸ç”¨æœ¯è¯­

### è¾“å‡ºè¦æ±‚
ä»…è¾“å‡ºç¿»è¯‘åŽçš„ä¸­æ–‡æ ‡ç­¾ï¼Œä¸è¦åŒ…å«ä»»ä½•æ ‡ç‚¹ç¬¦å·ã€è§£é‡Šæˆ–é¢å¤–è¯´æ˜Žã€‚"""
        else:
            sys_prompt = "You are a translator. Translate the following Chinese audio description to English. Output ONLY the English translation."
        
        try:
            service_config = AIServiceConfig(
                provider_id=config["provider"],
                api_key=api_key,
                base_url=config["base_url"],
                model_name=config["model"],
                system_prompt=sys_prompt,
                timeout=10,
                max_tokens=64,
                temperature=0.3
            )
            service = OpenAICompatibleService(service_config)
            
            # Run sync
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                result = loop.run_until_complete(service.translate_single(text))
                if result.success:
                    translated_text = result.data.strip()
                    return translated_text
            finally:
                loop.close()
                asyncio.run(service.cleanup())
        except Exception as e:
            logger.error(f"Translation failed: {e}")
        
        return None


class MusicGenDownloadWorker(BaseWorker):
    """
    Worker for downloading MusicGen FP16 models.
    """
    
    def __init__(self, parent: Optional[QObject] = None):
        super().__init__(parent)
    
    def run(self) -> None:
        try:
            from transcriptionist_v3.application.ai_engine.musicgen.downloader import MusicGenDownloader
            downloader = MusicGenDownloader()
            
            def progress_cb(filename: str, current: int, total: int):
                if self.is_cancelled:
                    downloader.cancel()
                    return
                    
                msg = f"ä¸‹è½½ä¸­: {filename}"
                if total > 0:
                    percent = int((current / total) * 100)
                    self.progress.emit(percent, 100, msg)
                else:
                    self.progress.emit(0, 0, msg)
                    
            try:
                downloader.download(progress_cb)
                
                if self.is_cancelled:
                    return
                    
                self.finished.emit(True)
                
            except Exception as e:
                if not self.is_cancelled:
                    logger.error(f"MusicGen download failed: {e}")
                    self.error.emit(str(e))
                
        except Exception as e:
            logger.error(f"Worker setup failed: {e}")
            self.error.emit(str(e))


class MusicGenGenerationWorker(BaseWorker):
    """
    Worker for generating audio using MusicGen.
    """
    
    def __init__(self, inference_engine, prompt: str, duration: int, parent: Optional[QObject] = None):
        super().__init__(parent)
        self.engine = inference_engine
        self.prompt = prompt
        self.duration = duration
        
    def run(self) -> None:
        try:
            def callback(percent, msg):
                if self.is_cancelled:
                    raise InterruptedError("Generation cancelled")
                self.progress.emit(percent, 100, msg)
                
            # Execute generation
            # Returns: (sample_rate, audio_data_numpy)
            result = self.engine.generate(self.prompt, self.duration, callback=callback)
            
            if self.is_cancelled:
                return
                
            self.finished.emit(result)
            
        except Exception as e:
            if not self.is_cancelled:
                logger.error(f"Generation failed: {e}")
                self.error.emit(str(e))
